
# MedGemma / PaliGemma Inference Server (Memory-Efficient Version)
# Upload this script to a Kaggle Notebook or Google Colab with GPU support.

import os
import sys
import subprocess
import json
from PIL import Image

# Ensure required libraries are installed
try:
    import torch
    import gradio as gr
    from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig
    from huggingface_hub import login
    import bitsandbytes as bnb
    from kaggle_secrets import UserSecretsClient
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "torch", "transformers", "gradio", "pillow", "accelerate", "huggingface_hub", "kaggle-secrets", "bitsandbytes"])
    import torch
    import gradio as gr
    from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig
    from huggingface_hub import login
    import bitsandbytes as bnb
    from kaggle_secrets import UserSecretsClient


# --- AUTHENTICATION ---
try:
    user_secrets = UserSecretsClient()
    HF_TOKEN = user_secrets.get_secret("HF_TOKEN")
    if HF_TOKEN:
        print("Logging in to Hugging Face with Secret Token...")
        login(token=HF_TOKEN)
    else:
        print("‚ö†Ô∏è Secret 'HF_TOKEN' not found. Using Mock mode.")
except Exception as e:
    print(f"Authentication Message: {e}")
    HF_TOKEN = None


# --- MODEL CONFIGURATION (With 8-bit Loading for Memory Savings) ---
MODEL_ID = "google/paligemma-3b-mix-224"
device = "cuda" if torch.cuda.is_available() else "cpu"

model = None
processor = None

if HF_TOKEN:
    print(f"Loading REAL model: {MODEL_ID}...")
    try:
        # 8-bit Quantization Config (Critical for Kaggle T4 GPU)
        quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        
        processor = AutoProcessor.from_pretrained(MODEL_ID)
        
        # Load model in 8-bit mode
        model = PaliGemmaForConditionalGeneration.from_pretrained(
            MODEL_ID,
            quantization_config=quantization_config,
            device_map="auto"
        )
        print("‚úÖ REAL Model loaded successfully (8-bit Quantized)!")
    except Exception as e:
        print(f"‚ùå Failed to load REAL model: {e}")
        print("Falling back to Mock Mode.")

def analyze_image(image, prompt):
    if image is None:
        return json.dumps({"error": "No image provided"})
    
    try:
        if prompt is None or prompt.strip() == "":
            prompt = "describe the findings in this chest x-ray including any abnormalities"

        decoded = ""
        clinical_context = ""
        ai_confidence = ""

        if model:
            # üß† REAL AI LOGIC
            image = image.convert("RGB")
            inputs = processor(text=prompt, images=image, return_tensors="pt").to("cuda") # Inputs to CUDA
            input_len = inputs["input_ids"].shape[-1]
            
            with torch.inference_mode():
                generation = model.generate(**inputs, max_new_tokens=200, do_sample=False)
                generation = generation[0][input_len:]
                decoded = processor.decode(generation, skip_special_tokens=True)
            
            ai_confidence = "High (Run on PaliGemma 8-bit)"
            clinical_context = "Analysis generated by Google PaliGemma-3b-mix (Quantized)."
        else:
            # üé≠ MOCK AI LOGIC (Fallback)
            decoded = f"Simulated Analysis: {prompt}. Findings suggest consolidation consistent with pneumonia. (Mock Mode)"
            ai_confidence = "Mock (Real Model Failed)"
            clinical_context = "Analysis generated by Mock Fallback."

        response = {
            "jointInterpretation": [
                decoded,
                f"AI Confidence: {ai_confidence}"
            ],
            "clinicalContext": clinical_context,
            "uncertainties": "Clinical correlation recommended.",
            "followUps": ["Verify findings with radiologist"],
            "isInconsistent": False
        }
        return json.dumps(response)

    except Exception as e:
        return json.dumps({"error": str(e)})

# Create Gradio Interface
demo = gr.Interface(
    fn=analyze_image,
    inputs=[
        gr.Image(type="pil", label="Upload X-Ray"),
        gr.Textbox(label="Prompt", value="describe the findings in this chest x-ray")
    ],
    outputs=gr.JSON(label="Analysis Result"),
    title="MedGemma AI Analysis",
    description="Backend for MedGemma App. Uses PaliGemma-3b-mix (8-bit) if token is present."
)

if __name__ == "__main__":
    print("Starting Gradio Server...")
    demo.launch(share=True, debug=True)
