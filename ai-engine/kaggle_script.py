
# MedGemma / PaliGemma Inference Server (Memory-Efficient Version)
# Upload this script to a Kaggle Notebook or Google Colab with GPU support.

import os
import sys
import subprocess
import json
from PIL import Image

# Ensure required libraries are installed
try:
    import torch
    import gradio as gr
    from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig
    from huggingface_hub import login
    import bitsandbytes as bnb
    from dotenv import load_dotenv
    load_dotenv()
    from kaggle_secrets import UserSecretsClient
    from pyngrok import ngrok, conf
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "torch", "transformers", "gradio", "pillow", "accelerate", "huggingface_hub", "bitsandbytes", "-U", "python-dotenv", "pyngrok"])
    import torch
    import gradio as gr
    from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig
    from huggingface_hub import login
    import bitsandbytes as bnb
    from dotenv import load_dotenv
    load_dotenv()
    from kaggle_secrets import UserSecretsClient
    from pyngrok import ngrok, conf


# --- AUTHENTICATION ---
# --- AUTHENTICATION ---
HF_TOKEN = os.getenv("HF_TOKEN")

if not HF_TOKEN:
    try:
        from kaggle_secrets import UserSecretsClient
        user_secrets = UserSecretsClient()
        HF_TOKEN = user_secrets.get_secret("HF_TOKEN")
        print("‚úÖ Found HF_TOKEN in Kaggle Secrets.")
    except Exception:
        print("‚ö†Ô∏è Kaggle Secret not found / Not running on Kaggle.")

# Fallback (Optional - ideally use .env)
if not HF_TOKEN:
    # Manual fallback if needed, otherwise leave None
    # HF_TOKEN = "hf_..." 
    pass
    
if HF_TOKEN:
    try:
        login(token=HF_TOKEN)
        print("Logged in to Hugging Face successfully.")
    except Exception as e:
        print(f"Login failed: {e}")
        HF_TOKEN = None
else:
    print("‚ö†Ô∏è No HF_TOKEN found. Using Mock mode.")


# --- MODEL CONFIGURATION (With 8-bit Loading for Memory Savings) ---
MODEL_ID = "google/paligemma-3b-mix-224"
device = "cuda" if torch.cuda.is_available() else "cpu"

model = None
processor = None

if HF_TOKEN:
    print(f"Loading REAL model: {MODEL_ID}...")
    try:
        # 8-bit Quantization Config (Critical for Kaggle T4 GPU)
        quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        
        processor = AutoProcessor.from_pretrained(MODEL_ID)
        
        # Load model in 8-bit mode
        model = PaliGemmaForConditionalGeneration.from_pretrained(
            MODEL_ID,
            quantization_config=quantization_config,
            device_map="auto"
        )
        print("‚úÖ REAL Model loaded successfully (8-bit Quantized)!")
    except Exception as e:
        print(f"‚ùå Failed to load REAL model: {e}")
        print("Falling back to Mock Mode.")

def analyze_image(image, prompt):
    if image is None:
        return json.dumps({"error": "No image provided"})
    
    try:
        if prompt is None or prompt.strip() == "":
            prompt = "describe the findings in this chest x-ray including any abnormalities"

        decoded = ""
        clinical_context = ""
        ai_confidence = ""

        if model:
            # üß† REAL AI LOGIC
            image = image.convert("RGB")
            inputs = processor(text=prompt, images=image, return_tensors="pt").to("cuda") # Inputs to CUDA
            input_len = inputs["input_ids"].shape[-1]
            
            with torch.inference_mode():
                generation = model.generate(**inputs, max_new_tokens=200, do_sample=False)
                generation = generation[0][input_len:]
                decoded = processor.decode(generation, skip_special_tokens=True)
            
            ai_confidence = "High (Run on PaliGemma 8-bit)"
            clinical_context = "Analysis generated by Google PaliGemma-3b-mix (Quantized)."
        else:
            # üé≠ MOCK AI LOGIC (Fallback)
            decoded = f"Simulated Analysis: {prompt}. Findings suggest consolidation consistent with pneumonia. (Mock Mode)"
            ai_confidence = "Mock (Real Model Failed)"
            clinical_context = "Analysis generated by Mock Fallback."

        response = {
            "jointInterpretation": [
                decoded,
                f"AI Confidence: {ai_confidence}"
            ],
            "clinicalContext": clinical_context,
            "uncertainties": "Clinical correlation recommended.",
            "followUps": ["Verify findings with radiologist"],
            "isInconsistent": False
        }
        return json.dumps(response)

    except Exception as e:
        return json.dumps({"error": str(e)})

# Create Gradio Interface
demo = gr.Interface(
    fn=analyze_image,
    inputs=[
        gr.Image(type="pil", label="Upload X-Ray"),
        gr.Textbox(label="Prompt", value="describe the findings in this chest x-ray")
    ],
    outputs=gr.JSON(label="Analysis Result"),
    title="MedGemma AI Analysis",
    description="Backend for MedGemma App. Uses PaliGemma-3b-mix (8-bit) if token is present."
)

if __name__ == "__main__":
    print("Starting Gradio Server...")
    
    # --- NGROK SETUP (Optional) ---
    # Attempt to get Ngrok token from Env or Kaggle Secrets
    NGROK_TOKEN = os.getenv("NGROK_AUTH_TOKEN")
    if not NGROK_TOKEN:
        try:
            NGROK_TOKEN = user_secrets.get_secret("NGROK_AUTH_TOKEN")
        except:
            pass
            
    # Fallback if not found
    if not NGROK_TOKEN:
        NGROK_TOKEN = "39OE32a2Rac9K1ehHHFFkRjtbLy_xBzApVQzTBhpJQUYyfmk"

    if NGROK_TOKEN:
        print("üîó Ngrok key found. Setting up ngrok tunnel...")
        conf.get_default().auth_token = NGROK_TOKEN
        # Open a tunnel to the default Gradio port
        public_url = ngrok.connect(7860).public_url
        print(f"‚úÖ Ngrok Tunnel Active: {public_url}")
        print(f"üëâ COPY THIS URL to your .env file as AI_SERVICE_URL")
        
        # Launch without sharing, since ngrok handles it
        demo.launch(share=False, debug=True, server_port=7860)
    else:
        print("‚ö†Ô∏è No NGROK_AUTH_TOKEN found. Falling back to default Gradio Share.")
        print("Note: Gradio Share links expire in 72 hours.")
        demo.launch(share=True, debug=True)
